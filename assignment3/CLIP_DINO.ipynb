{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7MV4XqiiQ0g"
   },
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "FOLDERNAME = \"cs231n/assignments/assignment3/\"\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0CPS6Gm27O8"
   },
   "outputs": [],
   "source": [
    "# This downloads the COCO dataset to your Drive if it doesn't already exist\n",
    "# (you should already have this dataset from a previous notebook!)\n",
    "# Uncomment the following if you don't have it.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# !bash get_coco_captioning.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7mkg1F0J081A",
    "ExecuteTime": {
     "end_time": "2026-01-10T12:58:46.690203800Z",
     "start_time": "2026-01-10T12:55:22.721627200Z"
    }
   },
   "source": [
    "# Some useful python libraries\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install decord"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: regex in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (2025.11.3)\n",
      "Requirement already satisfied: tqdm in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: wcwidth in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from ftfy) (0.2.14)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\lenovo\\appdata\\local\\temp\\pip-req-build-910bh5gz\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (25.0)\n",
      "Requirement already satisfied: regex in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (4.67.1)\n",
      "Requirement already satisfied: torch in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (2.9.1)\n",
      "Requirement already satisfied: torchvision in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from clip==1.0) (0.24.1)\n",
      "Requirement already satisfied: wcwidth in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from ftfy->clip==1.0) (0.2.14)\n",
      "Requirement already satisfied: filelock in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (3.6)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torch->clip==1.0) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Requirement already satisfied: numpy in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torchvision->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from torchvision->clip==1.0) (12.0.0)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\Lenovo\\AppData\\Local\\Temp\\pip-req-build-910bh5gz'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting decord\n",
      "  Downloading decord-0.6.0-py3-none-win_amd64.whl.metadata (422 bytes)\n",
      "Requirement already satisfied: numpy>=1.14.0 in f:\\anaconda\\anaconda\\envs\\fashion\\lib\\site-packages (from decord) (2.1.3)\n",
      "Downloading decord-0.6.0-py3-none-win_amd64.whl (24.7 MB)\n",
      "   ---------------------------------------- 0.0/24.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.3/24.7 MB 3.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.6/24.7 MB 4.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.4/24.7 MB 4.4 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.5/24.7 MB 4.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.2/24.7 MB 4.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 6.3/24.7 MB 4.3 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 6.8/24.7 MB 4.2 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.1/24.7 MB 4.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.4/24.7 MB 4.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 10.7/24.7 MB 4.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.1/24.7 MB 4.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 13.4/24.7 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.7/24.7 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.0/24.7 MB 4.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.7 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.4/24.7 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/24.7 MB 5.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/24.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/24.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.7/24.7 MB 4.8 MB/s  0:00:05\n",
      "Installing collected packages: decord\n",
      "Successfully installed decord-0.6.0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-IumfyxjUZT"
   },
   "source": [
    "# State-of-the-Art Pretrained Image Models\n",
    "\n",
    "In the previous exercise, you learned about [SimCLR](https://arxiv.org/abs/2002.05709) and how contrastive self-supervised learning can be used to learn meaningful image representations. In this notebook, we will explore two more recent models that also aim to learn high-quality visual representations and have demonstrated strong and robust performance on a variety of downstream tasks.\n",
    "\n",
    "\n",
    "First, we will examine the [CLIP](https://github.com/openai/CLIP) model. Like SimCLR, CLIP uses a contrastive learning objective, but instead of contrasting two augmented views of the same image, it contrasts two different modalities: text and image. To train CLIP, OpenAI collected a large dataset of ~400M image-text pairs from the internet, including sources like Wikipedia and image alt text. The resulting model learns rich, high-level image features and has achieved impressive zero-shot performance on many vision benchmarks.\n",
    "\n",
    "Next, we will explore [DINO](https://github.com/facebookresearch/dino), a self-supervised learning method for vision tasks that applies contrastive learning in a self-distillation framework with multi-crop augmentation strategy. The authors showed that the features learned by DINO ViTs are fine-grained and semantically rich with explicit information about the semantic segmentation of the image.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZJOAUKaoze_"
   },
   "source": [
    "# CLIP\n",
    "\n",
    "As explained above, CLIP's training objective incorporates both text and images, building upon the principles of contrastive learning. Consider this quote from the SimCLR notebook:\n",
    ">The goal of the contrastive loss is to maximize agreement between the final vectors **$z_i = g(h_i)$** and **$z_j = g(h_j)$**.\n",
    "\n",
    "Similarly, CLIP is trained to maximize agreement between two vectors. However, because these vectors come from different modalities, CLIP uses two separate encoders: a transformer-based Text Encoder and a Vision Transformer (ViT)-based Image Encoder. Note that some smaller, more efficient versions of CLIP use a ResNet as the Image Encoder instead of a ViT.\n",
    "\n",
    "Run the cell below to visualize the training and inference pipeline of CLIP.\n",
    "\n",
    "During the pretraining phase, each batch consists of multiple images along with their corresponding captions. Each image is independently processed by an Image Encoder—typically a visual model like a Vision Transformer (ViT) or a Convolutional Neural Network (ConvNet)—which produces an image embedding $I_n$. Likewise, each caption is independently processed by a Text Encoder to generate a corresponding text embedding $T_n$. Next, we compute the pairwise similarities between all image-text combinations, meaning each image is compared with every caption, and vice versa. The training objective is to maximize the similarity scores along the diagonal of the resulting similarity matrix -- that is, the scores for the matching image-caption pairs $(I_n, T_n)$.  Through backpropagation, the model learns to assign higher similarity scores to true matches than to mismatched pairs.\n",
    "\n",
    "Through this setup, CLIP effectively learns to represent images and texts in a shared latent space. In this space, semantic concepts are encoded in a modality-independent way, enabling meaningful cross-modal comparisons between visual and textual inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyNGHPpf4kIL"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ColabImage\n",
    "ColabImage(f'/content/drive/My Drive/{FOLDERNAME}/CLIP.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96UkICCbmoX-"
   },
   "source": [
    "**Inline Question 1** -\n",
    "\n",
    "Why does CLIP's learning depend on the batch size? If the batch size is fixed, what strategy can we use to learn rich image features?\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "CLIP 的对比学习目标依赖于批量中的所有图像-文本对来计算相似性矩阵。较大的批量大小提供更多的负样本，从而提高对比学习的效果\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkVe0nRuxXIP"
   },
   "source": [
    "# Loading COCO dataset\n",
    "\n",
    "We'll use the same captioning dataset you used to train your RNN captioning model, but instead of generating the captions lets see if we can match each image to the correct caption."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IUkEEQ2YyTSo",
    "ExecuteTime": {
     "end_time": "2026-01-10T13:38:20.779117200Z",
     "start_time": "2026-01-10T13:38:16.057355Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import clip\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "from cs231n.clip_dino import *\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\"Returns relative error.\"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-10, np.abs(x) + np.abs(y))))\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.protobuf'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtqdm\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mauto\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mPIL\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcs231n\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mclip_dino\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m *\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrel_error\u001B[39m(x, y):\n\u001B[32m     16\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Returns relative error.\"\"\"\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Pycharm\\pythonProject\\cs231assignments\\assignment3\\cs231n\\clip_dino.py:1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m device_v2\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtorch\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mnn\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnn\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001B[39m\n\u001B[32m     46\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tf2 \u001B[38;5;28;01mas\u001B[39;00m _tf2\n\u001B[32m     47\u001B[39m _tf2.enable()\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __operators__\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m audio\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:8\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[33;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m__internal__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m autograph\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m__internal__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m decorator\n\u001B[32m     10\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_api\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mv2\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m__internal__\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dispatch\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\autograph\\__init__.py:8\u001B[39m\n\u001B[32m      3\u001B[39m \u001B[33;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m_sys\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mag_ctx\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m control_status_ctx \u001B[38;5;66;03m# line: 34\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mimpl\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tf_convert \u001B[38;5;66;03m# line: 493\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\python\\autograph\\core\\ag_ctx.py:21\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01minspect\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mthreading\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ag_logging\n\u001B[32m     22\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutil\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtf_export\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tf_export\n\u001B[32m     25\u001B[39m stacks = threading.local()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\__init__.py:17\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[32m     14\u001B[39m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[32m     15\u001B[39m \u001B[33;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcontext_managers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m control_dependency_on_returns\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mmisc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m alias_tensors\n\u001B[32m     19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mautograph\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtensor_list\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m dynamic_list_append\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\python\\autograph\\utils\\context_managers.py:19\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[33;03m\"\"\"Various context managers.\"\"\"\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcontextlib\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mops\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor_array_ops\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcontrol_dependency_on_returns\u001B[39m(return_value):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mF:\\Anaconda\\anaconda\\envs\\fashion\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:32\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m typing \u001B[38;5;28;01mas\u001B[39;00m npt\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgoogle\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprotobuf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m message\n\u001B[32m     33\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m attr_value_pb2\n\u001B[32m     34\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtensorflow\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m full_type_pb2\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'google.protobuf'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jq0KZifyNDA"
   },
   "outputs": [],
   "source": [
    "from cs231n.coco_utils import load_coco_data, sample_coco_minibatch, decode_captions\n",
    "from cs231n.image_utils import image_from_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AT-Ux6N8ovqZ"
   },
   "outputs": [],
   "source": [
    "# Load COCO data from disk into a dictionary.\n",
    "# this is the same dataset you used for the RNN captioning notebook :)\n",
    "data = load_coco_data(pca_features=True)\n",
    "\n",
    "# Print out all the keys and values from the data dictionary.\n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZYimNYozKxh"
   },
   "outputs": [],
   "source": [
    "# we're just using the loaded captions from COCO, so we need to decode them and get rid of the special tokens.\n",
    "decoded_captions= []\n",
    "for caption in data['val_captions']:\n",
    "  caption = decode_captions(caption, data['idx_to_word'])\\\n",
    "    .replace('<START>', '')\\\n",
    "    .replace('<END>', '')\\\n",
    "    .replace('<UNK>', '')\\\n",
    "    .strip()\n",
    "  decoded_captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bdpT8K52ukP"
   },
   "outputs": [],
   "source": [
    "# lets get 10 examples\n",
    "mask = np.array([135428, 122586, 122814, 133173, 176639, 163828,  98169,   6931,\n",
    "        19488, 175760])\n",
    "first_captions = [decoded_captions[elem] for elem in mask]\n",
    "\n",
    "img_idxs = data['val_image_idxs'][mask]       # the images the captions refer to\n",
    "first_images   = [image_from_url(data['val_urls'][j]) for j in img_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-Zvmbg_N42T"
   },
   "outputs": [],
   "source": [
    "for i, (caption, image) in enumerate(zip(first_captions, first_images)):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    caption_str = caption\n",
    "    plt.title(caption_str)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I144vQb4sXVW"
   },
   "source": [
    "# Running the CLIP Model\n",
    "\n",
    "First we'll use the pretrained CLIP model to extract features from the texts and images separetely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNWKDxLs29Dd"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-inoZiP79TJ"
   },
   "outputs": [],
   "source": [
    "# You can check the model layers by printing the model.\n",
    "# CLIP's model code is available at https://github.com/openai/CLIP/tree/main/clip\n",
    "# print(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znkbk9r1zjkC"
   },
   "outputs": [],
   "source": [
    "# First, we encode the captions into vectors in the shared embedding space.\n",
    "# Since we're using a Transformer as the text encoder, we need to tokenize the text first.\n",
    "text_tokens = clip.tokenize(first_captions).to(device)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_tokens)\n",
    "\n",
    "# Sanity check, print the shape\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vvRlMrR2XZk"
   },
   "outputs": [],
   "source": [
    "# Then, we encode the images into the same embedding space.\n",
    "processed_images = [\n",
    "    clip_preprocess(Image.fromarray(img)).unsqueeze(0)\n",
    "    for img in first_images\n",
    "]\n",
    "images_tensor = torch.cat(processed_images, dim=0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(images_tensor)\n",
    "\n",
    "# sanity check, print the shape\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3IfwGD__Xb8"
   },
   "source": [
    "Open `cs231n/clip_dino.py` and implement `get_similarity_no_loop` to compute similarity scores between text features and image features. Test your implementation below, you should see relative errors less than 1e-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWRW5AG4_X94"
   },
   "outputs": [],
   "source": [
    "from cs231n.clip_dino import get_similarity_no_loop\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "M, N, D = 5, 6, 10\n",
    "\n",
    "test_text_features = torch.randn(N, D)\n",
    "test_image_features = torch.randn(M, D)\n",
    "out = get_similarity_no_loop(test_text_features, test_image_features)\n",
    "\n",
    "expected_out = np.array([\n",
    "    [ 0.1867811 , -0.23494351,  0.44155994, -0.18950461,  0.00100103],\n",
    "    [ 0.17905031, -0.25469488, -0.64330417,  0.25097957, -0.09327742],\n",
    "    [-0.4407011 , -0.4365381 ,  0.32857686, -0.3765278 ,  0.01049389],\n",
    "    [ 0.24815483,  0.42157224, -0.08459304,  0.14132318, -0.26935193],\n",
    "    [ 0.02309848, -0.01441101,  0.5469337 ,  0.6018773 ,  0.21581158],\n",
    "    [ 0.41579214, -0.014449  , -0.7242257 ,  0.39348006,  0.0822239 ],\n",
    "]).astype(np.float32)\n",
    "\n",
    "print(\"relative error: \", rel_error(out.numpy(), expected_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8t54B7xh4PL-"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the similarities between our batch of images and their captions.\n",
    "\n",
    "similarities = get_similarity_no_loop(text_features, image_features).cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(20, 14))\n",
    "plt.imshow(similarities, vmin=0.1, vmax=0.3)\n",
    "plt.yticks(range(len(text_features)), first_captions, fontsize=18)\n",
    "plt.xticks([])\n",
    "for i, image in enumerate(first_images):\n",
    "    plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n",
    "for x in range(similarities.shape[1]):\n",
    "    for y in range(similarities.shape[0]):\n",
    "        plt.text(x, y, f\"{similarities[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
    "\n",
    "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "    plt.gca().spines[side].set_visible(False)\n",
    "\n",
    "plt.xlim([-0.5, len(image_features) - 0.5])\n",
    "plt.ylim([len(text_features) + 0.5, -2])\n",
    "\n",
    "plt.title(\"Cosine similarity between text and image features\", size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnNuJaABp8Nl"
   },
   "source": [
    "# Zero Shot Classifier\n",
    "\n",
    "You will be able to see a high similarity between matching image-caption pairs above. We can leverage this property to design an image classifier that doesn't require any labeled data (i.e., a zero-shot classifier). Each class can be represented using an appropriate natural language description, and any input image will be classified into the class whose description has the highest similarity with the image in CLIP's embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lDbkKaQY8Gd"
   },
   "source": [
    "Implement `clip_zero_shot_classifier` in `cs231n/clip_dino.py` and test it below. You should be able to see the following predictions:\n",
    "\n",
    "['a person', 'an animal', 'an animal', 'food', 'a person', 'a landscape', 'other', 'other', 'other', 'a person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bM_WEqsYrCx9"
   },
   "outputs": [],
   "source": [
    "from cs231n.clip_dino import clip_zero_shot_classifier\n",
    "\n",
    "classes = [\"a person\", \"an animal\", \"food\", \"a landscape\", \"other\"]\n",
    "\n",
    "pred_classes = clip_zero_shot_classifier(\n",
    "    clip_model, clip_preprocess, first_images, classes, device)\n",
    "\n",
    "print(pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yW5xwHdWZlzl"
   },
   "source": [
    "Run the cell below to visualize the predictions. As you can see, CLIP offers a straightforward way to perform reasonable zero-shot classification across any class taxonomy.\n",
    "\n",
    "CLIP was the first model to outperform standard supervised training on ImageNet classification without using any ImageNet images or labels (The original CLIP paper has many such interesting experiments and analysis).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "om4mnnVXZKAL"
   },
   "outputs": [],
   "source": [
    "# Visualize the zero shot predictions\n",
    "for i, (pred_class, image) in enumerate(zip(pred_classes, first_images)):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(pred_class)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVCDz3tyfaAD"
   },
   "source": [
    "# Image Retrieval using CLIP\n",
    "\n",
    "Just as we used CLIP to retrieve the matching class name for each image, we can also use it to retrieve matching images from text inputs (semantic image retrieval). Implement the `CLIPImageRetriever` in `cs231n/clip_dino.py` and test it by running the two cells below. The expected top 2 outputs for each query are provided in the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "077RtVSAfaXd"
   },
   "outputs": [],
   "source": [
    "from cs231n.clip_dino import CLIPImageRetriever\n",
    "clip_retriever = CLIPImageRetriever(clip_model, clip_preprocess, first_images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "609IYhTzkMBF"
   },
   "outputs": [],
   "source": [
    "query = \"sports\"  # tennis, skateboard\n",
    "# query = \"black and white\"  # bathroom, zerbas\n",
    "img_indices = clip_retriever.retrieve(query)\n",
    "\n",
    "for img_index in img_indices:\n",
    "    plt.imshow(first_images[img_index])\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1gVu2VDogLb"
   },
   "source": [
    "**Inline Question 2** -\n",
    "\n",
    "CLIP learns to align image and text representations in a shared latent space using a contrastive loss. How would you extend this idea to more than two modalities?\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "为每种模态计算嵌入向量。\n",
    "计算所有模态之间的相似性矩阵。\n",
    "使用多模态对比损失，最大化匹配模态对的相似性，同时最小化非匹配模态对的相似性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otX-grqhmRAN"
   },
   "source": [
    "# DINO\n",
    "\n",
    "As mentioned earlier, models trained with vanilla contrastive learning methods such as SimCLR and CLIP require very large batch sizes. This makes them computationally expensive and limits their accessibility. Subsequent works, like [BYOL](https://arxiv.org/abs/2006.07733), propose an alternative approach that avoids the need for numerous negative samples by using a student-teacher framework. This method performs surprisingly well and was later adopted by [DINO](https://arxiv.org/abs/2104.14294) .\n",
    "\n",
    "Similar to SimCLR, DINO is trained to maximize the agreement between two vectors derived from different views of the same image. However, unlike SimCLR, DINO uses two separate encoders which are trained differently. The student network is updated via backpropagation to match the outputs of the teacher network. The teacher network is not updated via backpropagation; instead, its weights are updated using an exponential moving average (EMA) of the student's weights. This means that the teacher model evolves more slowly and provides a stable target for the student to learn from.\n",
    "\n",
    "Run the cell below to visualize the DINO training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35APMU0nq0Ja"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ColabImage\n",
    "ColabImage(f'/content/drive/My Drive/{FOLDERNAME}/dino.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuXuvIq8th9M"
   },
   "outputs": [],
   "source": [
    "# first let's get rid of the CLIP model that's currently using memory\n",
    "del clip_model\n",
    "# Uncomment the following if you are using GPU runtime\n",
    "# torch.cuda.empty_cache()\n",
    "# torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBM480L42ykK"
   },
   "outputs": [],
   "source": [
    "# Load smallest dino model. ViT-S/8. Here ViT-S has ~22M parameters and\n",
    "# works on 8x8 patches.\n",
    "dino_model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n",
    "dino_model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-35sU9tQ35KA"
   },
   "outputs": [],
   "source": [
    "# the image we will be playing around with\n",
    "sample_image = Image.fromarray(first_images[0]).convert(\"RGB\")\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ-tH24e6P0D"
   },
   "source": [
    "# DINO Attention Maps\n",
    "\n",
    "Since the loaded DINO checkpoint is based on the ViT architecture, we can visualize what each attention head is focusing on. The code below generates heatmaps showing which patches of the original image the [CLS] token attends to across the various heads in the final layer. Although this model was trained using a self-supervised objective without any explicit instruction to recognize \"structure\" in images, still...\n",
    "\n",
    "Do you notice any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux-p68vw4fdt"
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "from torchvision import transforms as T\n",
    "transform = T.Compose([\n",
    "    T.Resize((480, 480)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "img_tensor = transform(sample_image)\n",
    "w, h = img_tensor.shape[1:]\n",
    "img_tensor = img_tensor[None].to(device)\n",
    "\n",
    "# Extract attention\n",
    "with torch.no_grad():\n",
    "    attn = dino_model.get_last_selfattention(img_tensor)[0, :, 0, 1:]\n",
    "nh, tokens = attn.shape\n",
    "w_feat, h_feat = w // 8, h // 8\n",
    "attn = attn.reshape(nh, w_feat, h_feat)\n",
    "attn = torch.nn.functional.interpolate(attn.unsqueeze(0), scale_factor=8, mode=\"nearest\")[0].cpu().numpy()\n",
    "\n",
    "# Plot attention heads\n",
    "fig, axes = plt.subplots(1, nh, figsize=(3 * nh, 3))\n",
    "for i in range(nh):\n",
    "    ax = axes[i] if nh > 1 else axes\n",
    "    ax.imshow(attn[i], cmap='inferno')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMPaTwfB9e3Y"
   },
   "outputs": [],
   "source": [
    "# Extract patch token features and discard [CLS] token.\n",
    "with torch.no_grad():\n",
    "    all_tokens = dino_model.get_intermediate_layers(img_tensor, n=1)[0]  # (1, 1+N, D)\n",
    "    patch_tokens = all_tokens[:, 1:, :]  # (N, D)\n",
    "\n",
    "print(img_tensor.shape)\n",
    "print(all_tokens.shape)\n",
    "print(patch_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zQgCDpQ9r3p"
   },
   "source": [
    "\n",
    "**Inline Question 3**\n",
    "\n",
    "How do we get the tensor shapes printed above? Explain your answer.\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "img_tensor 是经过预处理的图像张量，形状为 (1, C, H, W)，其中 C 是通道数，H 和 W 是图像的高度和宽度。\n",
    "all_tokens 是模型输出的所有 token，包括 [CLS] token，形状为 (1, 1+N, D)，其中 N 是图像分块的数量，D 是每个 token 的维度。\n",
    "patch_tokens 是去掉 [CLS] token 后的分块 token，形状为 (1, N, D)。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG19KjdGCGp-"
   },
   "source": [
    "# DINO Features\n",
    "\n",
    "To understand what the model is encoding in each patch, we can visualize the contents of each patch token. Since these embeddings are high-dimensional and difficult to interpret directly, we'll use PCA to identify the directions of highest variance in the feature space.\n",
    "\n",
    "In the next cell, we visualize the three principal directions of variance in the feature space. This reveals the dominant structure that the patch embeddings are capturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YKYFpoa_XpS"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(231)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "patch_pca = pca.fit_transform(patch_tokens.cpu().numpy()[0])\n",
    "\n",
    "# Normalize PCA components to [0, 1] for RGB display\n",
    "patch_rgb = (patch_pca - patch_pca.min(0)) / (patch_pca.max(0) - patch_pca.min(0))\n",
    "\n",
    "# Reshape to image grid (60x60, 3)\n",
    "patch_rgb_img = patch_rgb.reshape(60, 60, 3)\n",
    "\n",
    "# Show as image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(patch_rgb_img)\n",
    "plt.axis('off')\n",
    "plt.title(\"Patch Embeddings (PCA → RGB)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgQR6poMAyOK"
   },
   "source": [
    "**Inline Question 4** -\n",
    "\n",
    "What kind of structure do you see in the visualization above? What does it imply when a region consistently appears in a specific color? What does it mean when two regions have distinctly different color? Remember that PCA reveals the directions of highest variance in the feature space across all patches. A patch's color reflects its distinct feature content.\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$一致颜色的区域表示这些区域的特征在高维空间中相似，捕获了相同的语义信息。\n",
    "不同颜色的区域表示这些区域的特征在高维空间中显著不同，捕获了不同的语义信息。\n",
    "这表明 PCA 提取的主成分能够揭示图像中显著的语义结构。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J7GZfBPFyFQ"
   },
   "source": [
    "# A Simple Segmentation Model over DINO Features\n",
    "\n",
    "In the previous section, we saw that DINO features can provide surprisingly good segmentation cues. Now, let's put that idea to the test by training a simple segmentation model on the [DAVIS dataset](https://davischallenge.org). The DAVIS dataset (Densely Annotated VIdeo Segmentation) was created for video object segmentation tasks. It provides frame-by-frame, pixel-level annotations of objects within videos. For this experiment, we'll train our model using the annotations from just a single frame of a video and see how well it performs on the remaining frames of the same.\n",
    "\n",
    "Our model will be intentionally minimal: we'll extract DINO features per patch and train a lightweight per-patch classifier using only the patches from that one annotated frame. Typically, you would train on the full dataset and evaluate on a separate validation set containing different videos. But here, we will test the one-shot capabilities of DINO features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6aAbgondcy2"
   },
   "outputs": [],
   "source": [
    "from cs231n.clip_dino import DavisDataset\n",
    "\n",
    "# A helper class to work with DAVIS dataset.\n",
    "# It may take ~5 minutes on the first run of this cell to download the dataset.\n",
    "davis_ds = DavisDataset()\n",
    "\n",
    "# Get a specific test video. Do NOT change this for submission.\n",
    "frames, masks = davis_ds.get_sample(7)\n",
    "num_classes = masks.max() + 1\n",
    "\n",
    "print(frames.shape, masks.shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yl0spxJRiAjf"
   },
   "outputs": [],
   "source": [
    "# Get DINO patch features and corresponding class labels for a middle frame\n",
    "train_fi = 40\n",
    "X_train = davis_ds.process_frames(frames[train_fi:train_fi+1], dino_model, device)[0]\n",
    "Y_train = davis_ds.process_masks(masks[train_fi:train_fi+1], device)[0]\n",
    "print(X_train.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6S3DAKl_9NE"
   },
   "source": [
    "Complete the implementation of the `DINOSegmentation` class in `cs231n/clip_dino.py`, and test it by running the two cells below. You should achieve a mean IoU greater than 0.45 on the first test frame and greater than 0.50 on the last test frame. To prevent overfitting on the training patch features, consider designing a very lightweight model (e.g., a linear layer or a 2-layer MLP) and applying appropriate weight decay.\n",
    "\n",
    "You may use GPU runtime to speed up training and evaluation. Make sure to rerun the entire notebook if you change runtime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B31JBmQm-T6"
   },
   "outputs": [],
   "source": [
    "from cs231n.clip_dino import DINOSegmentation, compute_iou\n",
    "torch.manual_seed(231)\n",
    "np.random.seed(231)\n",
    "dino_segmentation = DINOSegmentation(device, num_classes)\n",
    "dino_segmentation.train(X_train, Y_train, num_iters=500)\n",
    "\n",
    "\n",
    "# Test on first, middle, and last frame\n",
    "ious = []\n",
    "test_fis = [0, train_fi, 98]\n",
    "gt_viz = []\n",
    "pred_viz = []\n",
    "for fi in test_fis:\n",
    "  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n",
    "  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n",
    "  Y_pred = dino_segmentation.inference(X_test)\n",
    "  iou = compute_iou(Y_pred, Y_test, num_classes)\n",
    "  ious.append(iou)\n",
    "\n",
    "  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n",
    "  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n",
    "\n",
    "gt_viz = np.concatenate(gt_viz, 1)\n",
    "pred_viz = np.concatenate(pred_viz, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH5KOhQY4JCb",
    "test": "iou_accuracy"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean IoU on first test frames: {ious[0]:.3f}\")  # should be >0.45\n",
    "print(f\"Mean IoU on last test frames: {ious[2]:.3f}\")  # should be >0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_24lnmJ4FtvA"
   },
   "source": [
    "Now let's visualize the results. Run the two cells below to display the ground truth and predicted segmentation masks for the first, middle, and last frames. Note that the middle frame is part of the training set, while the other frames are unseen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qai7ItCa7P-D"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(gt_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxXEu--C7WYQ"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(pred_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmD1T5UoGVHu"
   },
   "source": [
    "Now run the following three cells to evaluate and visualize the entire video. You should achieve a mean IoU greater than 0.55. The saved visualization video may take some time to process in Google Drive, but you can download it to your computer and view it locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIRVNKs3oVPU"
   },
   "outputs": [],
   "source": [
    "# Run on all frames\n",
    "ious = []\n",
    "gt_viz = []\n",
    "pred_viz = []\n",
    "for fi in range(len(frames)):\n",
    "  if fi % 20 == 0:\n",
    "    print(f\"{fi} / {len(frames)}\")\n",
    "  X_test = davis_ds.process_frames(frames[fi:fi+1], dino_model, device)[0]\n",
    "  Y_test = davis_ds.process_masks(masks[fi:fi+1], device)[0]\n",
    "  Y_pred = dino_segmentation.inference(X_test)\n",
    "  iou = compute_iou(Y_pred, Y_test, num_classes)\n",
    "  ious.append(iou)\n",
    "\n",
    "  gt_viz.append(davis_ds.mask_frame_overlay(Y_test, frames[fi]))\n",
    "  pred_viz.append(davis_ds.mask_frame_overlay(Y_pred, frames[fi]))\n",
    "\n",
    "gt_viz = np.stack(gt_viz)  # T x H x W x 3\n",
    "pred_viz = np.stack(pred_viz)  # T x H x W x 3\n",
    "final_viz = np.concatenate([gt_viz, pred_viz], -2)  # T x H x 2W x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCZkYXtEGkq2",
    "test": "all_frames_iou"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean IoU on all frames: {sum(ious) / len(ious):.3f}\")  # should be >0.55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3DCRaiI8V7J"
   },
   "outputs": [],
   "source": [
    "def write_video_from_array(array, output_path, fps = 12):\n",
    "    T, H, W, _ = array.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (W, H))\n",
    "    for i in range(T):\n",
    "        frame = array[i]\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path}\")\n",
    "\n",
    "\n",
    "# It might take a while to process in google drive but you can just download it and watch on your computer\n",
    "write_video_from_array(final_viz, f\"/content/drive/My Drive/{FOLDERNAME}/dino_res.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5Z_N_lkHlPW"
   },
   "source": [
    "**Inline Question 5** -\n",
    "\n",
    "If you train a segmentation model on CLIP ViT's patch features, do you expect it to perform better or worse than DINO? Why should that be the case?\n",
    "\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "表现可能会更差，因为 CLIP 的特征是为跨模态对齐（图像-文本）而设计的，而不是为单模态任务（如分割）优化的。相比之下，DINO 的特征是通过自监督学习直接从图像中提取的，捕获了更细粒度的图像语义信息，更适合分割任务。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7joEne5LNYB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1veMUkShr38Pj6B_7XgEdtioxI6sVfXkU",
     "timestamp": 1747174681237
    },
    {
     "file_id": "1EAsfgn8wpiEIIubKR4KuQKxsdCp2rtY2",
     "timestamp": 1747090108952
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "name": "myenv",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
